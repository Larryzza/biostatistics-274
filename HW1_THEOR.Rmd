---
title: \textbf {BIOSTAT 274 Spring 2021 Homework 1}
subtitle: \textbf{Due 11:59 PM 04/21/2020 (Submit to CCLE)}
author: "Zian ZHUANG"
header-includes: \usepackage{amsgen,amsmath,amstext,amsbsy,amsopn,amssymb,mathabx,amsthm,bm,bbm}
urlcolor: blue
output:
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggplot2)
```

```{=tex}
\theoremstyle{definition}
\newtheorem*{hint}{Hint}
```
```{=tex}
\theoremstyle{remark}
\newtheorem*{rmk}{Remark}
```

## Theoretical Part

1.  

(a) Write down the optimization problem of general linear model, ridge regression and LASSO in estimating $\beta$ respectively.

**Answer**:

- As for general linear model, the optimization problem:

$$\begin{aligned} 
\hat{\beta}=\arg\min ||y-\beta||^2
\end{aligned} $$

- As for Ridge Regression, the optimization problem:

$$\begin{aligned} 
\hat{\beta}^R_\lambda=\arg\min ||y-\beta||^2 + \lambda||\beta||^2
\end{aligned} $$

In other words, we need to find 

$$\begin{aligned} 
&\text{minimize}_\beta \sum_{i=1}^n(y_i-\beta)^2\\
&\text{subject to }  \beta^2 \leq s
\end{aligned} $$

As for Lasso Regression, the optimization problem:

$$\begin{aligned} 
\hat{\beta}^L_\lambda=\arg\min ||y-\beta||^2 + \lambda||\beta||
\end{aligned} $$

In other words, we need to find 

$$\begin{aligned} 
&\text{minimize}_\beta \sum_{i=1}^n(y_i-\beta)^2\\
&\text{subject to }  \beta \leq s
\end{aligned} $$

(b) For fixed tuning parameter $\lambda$, solve for $\hat{\beta}$ (general linear model), $\hat{\beta}^R_\lambda$ (ridge regression) and $\hat{\beta}^L_\lambda$ (LASSO) respectively.

**Answer**:
- As for general linear model, we took derivative the loss function with respect to $\beta$ and set to zero, then we have

$$\begin{aligned} 
0=&(\sum_{i=1}^n(y_i-\hat{\beta})^2)^{'}\\
0=&\sum_{i=1}^n -2*y_i+2\hat{\beta}\\
0=&\sum_{i=1}^n -y_i+\hat{\beta}\\
n*\hat{\beta} =& \sum_{i=1}^n y_i\\
\hat{\beta} =& \frac{1}{n}\sum_{i=1}^n y_i=\bar{y_i}
\end{aligned} $$

- As for Ridge Regression, we took derivative the loss function with respect to $\beta$ and set to zero (fixed tuning parameter $\lambda$), then we have


$$\begin{aligned} 
0=&((y-X*\hat{\beta}^R)^T(y-X*\hat{\beta}^R) + \lambda||\hat{\beta}^R||^2)^{'},\text{(note that X is intercept 1)}\\
0=&-2\sum_{i=1}^n y_i+2n\hat{\beta}^R+2n\lambda\hat{\beta}^R\\
(1+\lambda)n*\hat{\beta}^R=& \bar{y_i}\\
\hat{\beta}^R =& \frac{1}{(1+1\lambda)}\bar{y_i}
\end{aligned} $$

- As for Lasso Regression, firstly we have, 

$$\begin{aligned} 
(y-\hat{\beta}^L_\lambda)^2=&(y-X\hat{\beta}_{ols}+X\hat{\beta}_{ols}-X\hat{\beta}^L_\lambda)^2,\text{(note that X is intercept 1)}\\
=&(y-X\hat{\beta}_{ols})^2+2*(y-X\hat{\beta}_{ols})^T*(X\hat{\beta}_{ols}-X\hat{\beta}^L_\lambda)+(\hat{\beta}_{ols}-X\hat{\beta}^L_\lambda)^2\\
=&(y-X\hat{\beta}_{ols})^2+(X\hat{\beta}_{ols}-X\hat{\beta}^L_\lambda)^2
\end{aligned} $$

Since $(y-\hat{\beta}_{ols})^2$ is not a function of $\hat{\beta}^L_\lambda$, we only need to minimize $(\hat{\beta}_{ols}-\hat{\beta}^L_\lambda)^2+ \lambda||\hat{\beta}||$ . We took derivative the loss function with respect to $\hat{\beta}^L_\lambda$ and set to zero (fixed tuning parameter $\lambda$) and plug in $\hat{\beta}_{ols}=\bar{y_i}$, then we have


$$\begin{aligned} 
0=&((X\hat{\beta}_{ols}-X\hat{\beta}^L_\lambda)^2+ \lambda||\hat{\beta}^L_\lambda||)^{'}\\
0=&((\hat{\beta}_{ols}-\hat{\beta}^L_\lambda)^T(\hat{\beta}_{ols}-\hat{\beta}^L_\lambda)+ \lambda||\hat{\beta}^L_\lambda||)^{'}\\
0=&((\hat{\beta}_{ols}-\hat{\beta}^L_\lambda)^2+\lambda||\hat{\beta}^L_\lambda||)^{'}\\
0=&-2\hat{\beta}_{ols}+2\hat{\beta}^L_\lambda+\lambda*\text{sign}(\hat{\beta}^L_\lambda)\\
2(\hat{\beta}_{ols}-\hat{\beta}^L_\lambda)=&\lambda*\text{sign}(\hat{\beta}^L_\lambda)\\
\hat{\beta}^L_\lambda=&\hat{\beta}_{ols}-\frac{\text{sign}(\hat{\beta}^L_\lambda)\lambda}{2}
\end{aligned} $$

Then we can have, 

$$ \hat{\beta}^L_\lambda=\left\{
\begin{aligned}
&\hat{\beta}_{ols}-\frac{\lambda}{2} ,\hat{\beta}_{ols}\geq \frac{\lambda}{2}\\
&0,\hat{\beta}_{ols}\in (-\frac{\lambda}{2},\frac{\lambda}{2})\\
&\hat{\beta}_{ols}+\frac{\lambda}{2} ,\hat{\beta}_{ols}\leq -\frac{\lambda}{2} 
\end{aligned}
\right.\Longrightarrow
\left\{
\begin{aligned}
&\bar{y_i}-\frac{\lambda}{2},\bar{y_i}\geq \frac{\lambda}{2}\\
&0,\bar{y_i}\in (-\frac{\lambda}{2},\frac{\lambda}{2})\\
&\bar{y_i}+\frac{\lambda}{2} ,\bar{y_i}\leq -\frac{\lambda}{2} 
\end{aligned}
\right.$$

(c) Represent $\hat{\beta}^R_\lambda$ and $\hat{\beta}^L_\lambda$ by $\hat{\beta}$ and create plots of them separately for $\lambda=1,5,10$ . What can you tell?

**Answer**:
we have 
$$ \begin{aligned}
\hat{\beta}^R =& \frac{1}{(1+1\lambda)}\hat{\beta}\\
\hat{\beta}^L_\lambda=&\left\{
\begin{aligned}
\hat{\beta}-\frac{\lambda}{2} ,\hat{\beta}\geq \frac{\lambda}{2}\\
\hat{\beta}+\frac{\lambda}{2} ,\hat{\beta}< \frac{\lambda}{2} 
\end{aligned}
\right.
\end{aligned}$$

```{r}
# generate beta
set.seed(199609)
beta <- seq(-10,10,0.01)
beta_R_1 <- 1/(1+1)*beta 
beta_R_5 <- 1/(1+5)*beta 
beta_R_10 <- 1/(1+10)*beta 

.trans <- function(x,lam){
  if(x>=lam/2){
  x-lam/2
}else if(x<=-lam/2){
  x+lam/2
}else{0}
}
beta_L_1 <- apply(beta %>% as.matrix,1,.trans,lam=1)
beta_L_2 <- apply(beta %>% as.matrix,1,.trans,lam=5)
beta_L_10 <- apply(beta %>% as.matrix,1,.trans,lam=10)

plots <- rbind(data.frame(beta_hat=beta, beta_est=beta_R_1, type="beta_R_1"),
      data.frame(beta_hat=beta, beta_est=beta_R_5, type="beta_R_5"),
      data.frame(beta_hat=beta, beta_est=beta_R_10, type="beta_R_10"),
      data.frame(beta_hat=beta, beta_est=beta_L_1, type="beta_L_1"),
      data.frame(beta_hat=beta, beta_est=beta_L_2, type="beta_L_2"),
      data.frame(beta_hat=beta, beta_est=beta_L_10, type="beta_L_10"))

ggplot(plots)+
  geom_line(aes(x=beta_hat,y=beta_est, group=type, color=type))+
  theme_bw()


```
We can tell from the plot that the a larger $\lambda$ will shrink the estimated coefficient $\beta$ closer to zero in both of Ridge and lasso regression. The difference is that the Ridge regression shrinks the coefficients towards 0, but lasso can shrink the coefficients to exact 0.

2. 

(a) Write out the ridge regression optimization problem in this setting.

**Answer**:

Ridge regression optimization problem is to minimize, 

$$ \begin{aligned}
(y_1 - \hat\beta_1x_{1} - \hat\beta_2x_{1})^2 + (y_2 - \hat\beta_1x_{2} - \hat\beta_2x_{2})^2 + \lambda (\hat\beta_1^2 + \hat\beta_2^2)
\end{aligned}$$

(b) Argue that the ridge coefficient estimates satisfy $\hat{\beta}^R_{\lambda1}=\hat{\beta}^R_{\lambda2}$

**Answer**:
we took derivative the loss function with respect to $\hat{\beta}^R_{\lambda1}$ and set to zero (fixed tuning parameter $\lambda$), then we have

$$ \begin{aligned}
0=&\frac{\partial }{\partial \hat{\beta}^R_{\lambda1}}((y_1 - \hat{\beta}^R_{\lambda1} x_{1} - \hat{\beta}^R_{\lambda2} x_{1})^2 + (y_2 - \hat{\beta}^R_{\lambda1} x_{2} - \hat{\beta}^R_{\lambda2} x_{2})^2 + \lambda (\hat{\beta}^{R2}_{\lambda1} + \hat{\beta}^{R2}_{\lambda2}))\\
  0=&(2\hat{\beta}^R_{\lambda1}x_{1}^2-2x_{1}y_1+2\hat{\beta}^R_{\lambda2}x_{1}^2) + (2\hat{\beta}^R_{\lambda1}x_{2}^2-2x_{2}y_2+2\hat{\beta}^R_{\lambda2}x_{2}^2) + 2\lambda\hat{\beta}^R_{\lambda1}\\
0=&(\hat{\beta}^R_{\lambda1}x_1^2-x_1y_1+\hat{\beta}^R_{\lambda2}x_1^2) + (\hat{\beta}^R_{\lambda2}x_2^2-x_2y_2+\hat{\beta}^R_{\lambda2}x_2^2) + \lambda\hat{\beta}^R_{\lambda1}\\
x_1y_1 + x_2y_2=&\hat{\beta}^R_{\lambda1} (x_1^2+x_2^2) + \hat{\beta}^R_{\lambda2} (x_1^2+x_2^2) + \lambda\hat{\beta}^R_{\lambda1}\\
x_1y_1 + x_2y_2=& (\hat{\beta}^R_{\lambda1}+ \hat{\beta}^R_{\lambda2})(x_1^2+x_2^2) + \lambda\hat{\beta}^R_{\lambda1}
\end{aligned}$$

Then we add $2x_1x_2\hat{\beta}^R_{\lambda1}$ and $2x_1x_2\hat{\beta}^R_{\lambda2}$ at both sides of the equation,
$$ \begin{aligned}
x_1y_1 + x_2y_2 + 2\hat{\beta}^R_{\lambda1}x_1x_2 + 2\hat{\beta}^R_{\lambda2}x_1x_2 =& (\hat{\beta}^R_{\lambda1}+ \hat{\beta}^R_{\lambda2}) (x_1^2 + x_2^2 + 2x_1x_2) + \lambda\hat{\beta}^R_{\lambda1}\\
x_1y_1 + x_2y_2 + 2\hat{\beta}^R_{\lambda1}x_1x_2 + 2\hat{\beta}^R_{\lambda2}x_1x_2 =& (\hat{\beta}^R_{\lambda1}+ \hat{\beta}^R_{\lambda2})(x_1+x_2)^2+ \lambda\hat{\beta}^R_{\lambda1}\\
x_1y_1 + x_2y_2 + 2\hat{\beta}^R_{\lambda1}x_1x_2 + 2\hat{\beta}^R_{\lambda2}x_1x_2 =& \lambda\hat{\beta}^R_{\lambda1},\text{(Given that }x_1+x_2=0 )
\end{aligned}$$

Similarly, we took derivative the function with respect to $\hat{\beta}^R_{\lambda2}$, then we have 

$$ \begin{aligned}
\lambda\hat{\beta}^R_{\lambda2} =& x_1y_1 + x_2y_2 + 2\hat{\beta}^R_{\lambda1}x_1x_2 + 2\hat{\beta}^R_{\lambda2}2x_1x_2
\end{aligned}$$

Thus we know that, 

$$ \begin{aligned}
\lambda\hat{\beta}^R_{\lambda1} =& x_1y_1 + x_2y_2 + 2\hat{\beta}^R_{\lambda1}x_1x_2 + 2\hat{\beta}^R_{\lambda2}x_1x_2\\
=& \lambda\hat{\beta}^R_{\lambda2}\\
\Longrightarrow \hat{\beta}^R_{\lambda1}=&\hat{\beta}^R_{\lambda2}=\frac{x_1y_1 + x_2y_2}{\lambda-4x_1x_2}
\end{aligned}$$

(c) Write out the LASSO optimization problem in this setting.

**Answer**:

LASSO regression optimization problem is to minimize, 

$$ \begin{aligned}
(y_1 - \hat{\beta}^L_{\lambda1}x_1 - \hat{\beta}^L_{\lambda2}x_1)^2 + (y_2 - \hat{\beta}^L_{\lambda1}x_2 - \hat{\beta}^L_{\lambda1}x_2)^2 + \lambda (|\hat{\beta}^L_{\lambda1}| + |\hat{\beta}^L_{\lambda2}|)
\end{aligned}$$

(d) (Optional) Argue that in this setting, the LASSO coefficients $\hat{\beta}^L_{\lambda1}$ and  $\hat{\beta}^L_{\lambda2}$ are not unique. Describe these solutions.

**Answer**:

Firstly, we took derivative the loss function with respect to $\hat{\beta}^L_{\lambda1}$ and set to zero (fixed tuning parameter $\lambda$), then we have

$$ \begin{aligned}
0=&\frac{\partial }{\partial \hat{\beta}^L_{\lambda1}}((y_1 - \hat{\beta}^L_{\lambda1} x_{1} - \hat{\beta}^L_{\lambda2} x_{1})^2 + (y_2 - \hat{\beta}^L_{\lambda1} x_{2} - \hat{\beta}^L_{\lambda2} x_{2})^2 \\
  0=&(2\hat{\beta}^L_{\lambda1}x_{1}^2-2x_{1}y_1+2\hat{\beta}^L_{\lambda2}x_{1}^2) + (2\hat{\beta}^L_{\lambda1}x_{2}^2-2x_{2}y_2+2\hat{\beta}^L_{\lambda2}x_{2}^2)\\
0=&(\hat{\beta}^L_{\lambda1}x_1^2-x_1y_1+\hat{\beta}^L_{\lambda2}x_1^2) + (\hat{\beta}^L_{\lambda2}x_2^2-x_2y_2+\hat{\beta}^L_{\lambda2}x_2^2)\\
x_1y_1 + x_2y_2=&\hat{\beta}^L_{\lambda1} (x_1^2+x_2^2) + \hat{\beta}^L_{\lambda2} (x_1^2+x_2^2)\\
x_1y_1 + x_2y_2=& (\hat{\beta}^L_{\lambda1}+ \hat{\beta}^L_{\lambda2})(x_1^2+x_2^2)\\
x_1y_1 + x_2y_2 + 2\hat{\beta}^L_{\lambda1}x_1x_2 + 2\hat{\beta}^L_{\lambda2}x_1x_2 =& (\hat{\beta}^L_{\lambda1}+ \hat{\beta}^L_{\lambda2}) (x_1^2 + x_2^2 + 2x_1x_2)\\
x_1y_1 + x_2y_2 + 2\hat{\beta}^L_{\lambda1}x_1x_2 + 2\hat{\beta}^L_{\lambda2}x_1x_2 =& (\hat{\beta}^L_{\lambda1}+ \hat{\beta}^L_{\lambda2})(x_1+x_2)^2\\
x_1y_1 + x_2y_2 + 2\hat{\beta}^L_{\lambda1}x_1x_2 + 2\hat{\beta}^L_{\lambda2}x_1x_2 =&  0,\text{(Given that }x_1+x_2=0 )\\
 2(\hat{\beta}^L_{\lambda1}+\hat{\beta}^L_{\lambda2})x_1x_2=&  -(x_1y_1 + x_2y_2)\\
 \hat{\beta}^L_{\lambda1}+\hat{\beta}^L_{\lambda2}=&-\frac{x_1y_1 + x_2y_2}{x_1x_2*2}
\end{aligned}$$

Since we know that $x_1+x_2=y_1+y_2=0$, we have,

$$ \begin{aligned}
\hat{\beta}^L_{\lambda1}+\hat{\beta}^L_{\lambda2}=&-\frac{x_1y_1 + x_2y_2}{x_1x_2*2}\\
=&-\frac{2x_1y_1}{2x_1x_2}\\
=&-\frac{y_1}{x_2}\\
=&\frac{y_1}{x_1}
\end{aligned}$$

We need to find the point at which contour of the objective function $\frac{y_1}{x_1}$ touch the the constraint set. Since we know that the Lasso constraints for $\hat{\beta}^L_{\lambda1}$ and  $\hat{\beta}^L_{\lambda2}$ is the area $|\hat{\beta}^L_{\lambda1}|+|\hat{\beta}^L_{\lambda2}|\leq s$ ($s$ is a constant) and objective function is $\frac{y_1}{x_1}$, then we found that the objective function is parallel with the edge of the Lasso constraints area $|\hat{\beta}^L_{\lambda1}|+|\hat{\beta}^L_{\lambda2}|\leq s$. Thus, the intersections  between the the objective function and the Lasso constraints area are more than one, i.e., all points on one edge of Lasso constraints area are able to optimize the objective function $\frac{y_1}{x_1}$.

Thus,the LASSO coefficients $\hat{\beta}^L_{\lambda1}$ and  $\hat{\beta}^L_{\lambda2}$ are not unique. The possible values of $\hat{\beta}^L_{\lambda1}$ and  $\hat{\beta}^L_{\lambda2}$ meet:

$$ \hat{\beta}^L_\lambda=\left\{
\begin{aligned}
\hat{\beta}^L_{\lambda1}+\hat{\beta}^L_{\lambda2}=& s, \hat{\beta}^L_{\lambda2}\geq0,\hat{\beta}^L_{\lambda2}\geq0 \\
\hat{\beta}^L_{\lambda1}+\hat{\beta}^L_{\lambda2}=& -s , \hat{\beta}^L_{\lambda2}<0,\hat{\beta}^L_{\lambda2}<0 
\end{aligned}
\right.$$

